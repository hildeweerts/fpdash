{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hildeweerts/anaconda/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# shap\n",
    "import shap \n",
    "\n",
    "# Directories\n",
    "data_dir = os.getcwd() + '/data'\n",
    "code_dir = os.getcwd() + '/fpdash'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# custom modules\n",
    "from cbr import prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = 1590 # adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing...\n",
      "...Sampled 3000 samples from dataset 1590.\n",
      "...Filled missing values.\n",
      "...Decoded to original feature values.\n",
      "...Scaled data.\n",
      "Preprocessing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hildeweerts/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/hildeweerts/Projects/fpdash/fpdash/cbr/prep.py:78: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  X_train = pd.DataFrame(scaler.transform(X_train), columns=list(X_train))\n",
      "/Users/hildeweerts/Projects/fpdash/fpdash/cbr/prep.py:79: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  X_test = pd.DataFrame(scaler.transform(X_test), columns=list(X_test))\n"
     ]
    }
   ],
   "source": [
    "data = prep.openmlwrapper(data_id=data_id, random_state=1, n_samples = 3000, verbose=True, scale=True, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test data further\n",
    "* train: \n",
    "    - true class known\n",
    "    - used during training\n",
    "* test: \n",
    "    - true class known\n",
    "    - not used during training\n",
    "* application: \n",
    "    - true class \"unknown\"\n",
    "    - not used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X_test_pre'], data['X_test_post'], data['y_test_pre'], data['y_test_post'] = train_test_split(data['X_test'], \n",
    "                                                                                                    data['y_test'].reset_index(drop=True), \n",
    "                                                                                                    random_state=1,\n",
    "                                                                                                    test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:    1.00\n",
      "Test Accuracy:        0.86\n",
      "\n",
      "Application Accuracy: 0.835\n",
      "Application AUC:      0.891\n",
      "Application Brier:    0.113\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 100, n_jobs=-2, random_state=1)\n",
    "clf.fit(data['X_train'], np.array(data['y_train']).ravel())\n",
    "print('Training Accuracy:    %.2f' % clf.score(data['X_train'], data['y_train']))\n",
    "print('Test Accuracy:        %.2f' % clf.score(data['X_test_pre'], data['y_test_pre']))\n",
    "print()\n",
    "print('Application Accuracy: %.3f' % clf.score(data['X_test_post'], data['y_test_post']))\n",
    "y_app_score = [i[1] for i in clf.predict_proba(data['X_test_post'])]\n",
    "print('Application AUC:      %.3f' % roc_auc_score(y_true=data['y_test_post']['class'].ravel(), y_score=y_app_score))\n",
    "print('Application Brier:    %.3f' % brier_score_loss(y_true=data['y_test_post']['class'].ravel(), y_prob=y_app_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create casebase and alerts\n",
    "* The case base consists of instances from the training dataset and test dataset.\n",
    "* The alert data consists of instances from the application dataset for which the model predicted a positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_indices = data['X_test_pre'].index\n",
    "post_indices = data['X_test_post'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Base\n",
    "data['X_base'] = pd.concat([data['X_train'], data['X_test_pre']]).reset_index(drop=True)\n",
    "data['y_base'] = pd.concat([data['y_train'], data['y_test_pre']]).reset_index(drop=True)\n",
    "data['X_base_decoded'] = pd.concat([data['X_train_decoded'], \n",
    "                                    data['X_test_decoded'].reset_index(drop=True).iloc[pre_indices]]\n",
    "                                  ).reset_index(drop=True)\n",
    "\n",
    "# Alerts\n",
    "y_test_post_pred = pd.DataFrame({'prediction' : clf.predict(data['X_test_post'])})\n",
    "y_test_post_pred['index'] = data['y_test_post'].index\n",
    "y_test_post_pred = y_test_post_pred.set_index('index')\n",
    "\n",
    "alert_indices = y_test_post_pred[y_test_post_pred['prediction']==1].index\n",
    "#alert_indices = y_test_post_pred.index\n",
    "data['X_alert'] = data['X_test_post'].copy().loc[alert_indices].reset_index(drop=True)\n",
    "data['y_alert'] = data['y_test_post'].copy().loc[alert_indices].reset_index(drop=True)\n",
    "data['X_alert_decoded'] = data['X_test_decoded'].reset_index(drop=True).loc[alert_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve metadata\n",
    "* Retrieve prediction probabilities (case base + alerts)\n",
    "* Retrieve historical performance (case base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction probabilities\n",
    "y_base_score = [i[1] for i in clf.predict_proba(data['X_base'])]\n",
    "y_alert_score = [i[1] for i in clf.predict_proba(data['X_alert'])]\n",
    "\n",
    "# Compute performance for cases in de case base\n",
    "y_base_pred = clf.predict(data['X_base'])\n",
    "base_performance = []\n",
    "for pred, true in zip(y_base_pred, data['y_base'].values.ravel()):\n",
    "    if (pred==1) and (true==1):\n",
    "        base_performance.append('TP')\n",
    "    elif (pred==1) and (true==0):\n",
    "        base_performance.append('FP')\n",
    "    elif (pred==0) and (true==0):\n",
    "        base_performance.append('TN')\n",
    "    elif (pred==0) and (true==1):\n",
    "        base_performance.append('FN')\n",
    "\n",
    "# gather metadata\n",
    "meta_base = pd.DataFrame({'performance' : base_performance, 'score' : y_base_score})\n",
    "meta_alert = pd.DataFrame({'score' : y_alert_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained.\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "SHAP_base = pd.DataFrame(explainer.shap_values(X=data['X_base'])[1], columns=list(data['X_base']))\n",
    "SHAP_alert = pd.DataFrame(explainer.shap_values(X=data['X_alert'])[1], columns=list(data['X_alert']))\n",
    "print('Explained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized nearest neighbor.\n"
     ]
    }
   ],
   "source": [
    "# Find nearest SHAP neighbors\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='brute', metric='euclidean', n_jobs=1)\n",
    "nn.fit(SHAP_base)\n",
    "print('Initialized nearest neighbor.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set jobs to 1 \n",
    "clf.set_params(n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save classifier\n",
    "with open(os.getcwd() + '/data/clf.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# Save nearest neighbor on SHAP\n",
    "with open(os.getcwd() + '/data/nn.pickle', 'wb') as handle:\n",
    "    pickle.dump(nn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Save case base\n",
    "data['X_base'].to_csv(os.getcwd() + '/data/X_base.csv', index=False)\n",
    "data['X_base_decoded'].to_csv(os.getcwd() + '/data/X_base_decoded.csv', index=False)\n",
    "meta_base.to_csv(os.getcwd() + '/data/meta_base.csv', index=False)\n",
    "SHAP_base.to_csv(os.getcwd() + '/data/SHAP_base.csv', index=False)\n",
    "data['y_base'].to_csv(os.getcwd() + '/data/y_base.csv', index=False)\n",
    "\n",
    "# Save alerts\n",
    "data['X_alert'].to_csv(os.getcwd() + '/data/X_alert.csv', index=False)\n",
    "data['X_alert_decoded'].to_csv(os.getcwd() + '/data/X_alert_decoded.csv', index=False)\n",
    "meta_alert.to_csv(os.getcwd() + '/data/meta_alert.csv', index=False)\n",
    "SHAP_alert.to_csv(os.getcwd() + '/data/SHAP_alert.csv', index=False)\n",
    "data['y_alert'].to_csv(os.getcwd() + '/data/y_alert.csv', index=False)\n",
    "\n",
    "# Save training data separately\n",
    "data['X_train'].to_csv(os.getcwd() + '/data/X_train.csv', index=False)\n",
    "\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
