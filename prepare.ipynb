{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hildeweerts/anaconda/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# shap\n",
    "import shap \n",
    "\n",
    "# Directories\n",
    "data_dir = os.getcwd() + '/data'\n",
    "code_dir = os.getcwd() + '/fpdash'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# custom modules\n",
    "from cbr import prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing...\n",
      "...Used all 1000 samples from dataset 31.\n",
      "...Filled missing values.\n",
      "...Decoded to original feature values.\n",
      "...Scaled data.\n",
      "Preprocessing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hildeweerts/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/hildeweerts/Projects/fpdash/fpdash/cbr/prep.py:78: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  X_train = pd.DataFrame(scaler.transform(X_train), columns=list(X_train))\n",
      "/Users/hildeweerts/Projects/fpdash/fpdash/cbr/prep.py:79: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  X_test = pd.DataFrame(scaler.transform(X_test), columns=list(X_test))\n"
     ]
    }
   ],
   "source": [
    "data = prep.openmlwrapper(data_id=31, random_state=1, n_samples = 2000, verbose=True, scale=True, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test data further\n",
    "* train: \n",
    "    - true class known\n",
    "    - used during training\n",
    "* test pre: \n",
    "    - true class known\n",
    "    - not used during training\n",
    "* test post: \n",
    "    - true class unknown\n",
    "    - not used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X_test_pre'], data['X_test_post'], data['y_test_pre'], data['y_test_post'] = train_test_split(data['X_test'], \n",
    "                                                                                                    data['y_test'], \n",
    "                                                                                                    random_state=1,\n",
    "                                                                                                    test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:    1.00\n",
      "Test (pre) Accuracy:  0.74\n",
      "Test (post) Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 100, n_jobs=-2, random_state=1)\n",
    "clf.fit(data['X_train'], np.array(data['y_train']).ravel())\n",
    "print('Training Accuracy:    %.2f' % clf.score(data['X_train'], data['y_train']))\n",
    "print('Test (pre) Accuracy:  %.2f' % clf.score(data['X_test_pre'], data['y_test_pre']))\n",
    "print('Test (post) Accuracy: %.2f' % clf.score(data['X_test_post'], data['y_test_post']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create casebase and alerts\n",
    "* The case base consists of instances from the training dataset and test-pre dataset.\n",
    "* The alert data consists of instances from test-post dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_indices = data['X_test_pre'].index\n",
    "post_indices = data['X_test_post'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Base\n",
    "data['X_base'] = pd.concat([data['X_train'], data['X_test_pre']]).reset_index(drop=True)\n",
    "data['y_base'] = pd.concat([data['y_train'], data['y_test_pre']]).reset_index(drop=True)\n",
    "data['X_base_decoded'] = pd.concat([data['X_train_decoded'], data['X_test_decoded'].iloc[pre_indices]]).reset_index(drop=True)\n",
    "\n",
    "# Alerts\n",
    "data['X_alert'] = data['X_test_post'].copy().reset_index(drop=True)\n",
    "data['y_alert'] = data['y_test_post'].copy().reset_index(drop=True)\n",
    "data['X_alert_decoded'] = data['X_test_decoded'].iloc[post_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve metadata\n",
    "* Retrieve prediction probabilities (case base + alerts)\n",
    "* Retrieve historical performance (case base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction probabilities\n",
    "y_base_score = [i[1] for i in clf.predict_proba(data['X_base'])]\n",
    "y_alert_score = [i[1] for i in clf.predict_proba(data['X_alert'])]\n",
    "\n",
    "# Compute performance for cases in de case base\n",
    "y_base_pred = clf.predict(data['X_base'])\n",
    "base_performance = []\n",
    "for pred, true in zip(y_base_pred, data['y_base'].values.ravel()):\n",
    "    if (pred==1) and (true==1):\n",
    "        base_performance.append('TP')\n",
    "    elif (pred==1) and (true==0):\n",
    "        base_performance.append('FP')\n",
    "    elif (pred==0) and (true==0):\n",
    "        base_performance.append('TN')\n",
    "    elif (pred==0) and (true==1):\n",
    "        base_performance.append('FN')\n",
    "\n",
    "# gather metadata\n",
    "meta_base = pd.DataFrame({'performance' : base_performance, 'score' : y_base_score})\n",
    "meta_alert = pd.DataFrame({'score' : y_alert_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained.\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "SHAP_base = pd.DataFrame(explainer.shap_values(X=data['X_base'])[1], columns=list(data['X_base']))\n",
    "SHAP_alert = pd.DataFrame(explainer.shap_values(X=data['X_alert'])[1], columns=list(data['X_alert']))\n",
    "print('Explained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized nearest neighbor.\n"
     ]
    }
   ],
   "source": [
    "# Find nearest SHAP neighbors\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='brute', metric='euclidean', n_jobs=1)\n",
    "nn.fit(SHAP_base)\n",
    "print('Initialized nearest neighbor.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set jobs to 1 \n",
    "clf.set_params(n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save classifier\n",
    "with open(os.getcwd() + '/data/clf.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# Save nearest neighbor on SHAP\n",
    "with open(os.getcwd() + '/data/nn.pickle', 'wb') as handle:\n",
    "    pickle.dump(nn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Save case base\n",
    "data['X_base'].to_csv(os.getcwd() + '/data/X_base.csv', index=False)\n",
    "data['X_base_decoded'].to_csv(os.getcwd() + '/data/X_base_decoded.csv', index=False)\n",
    "meta_base.to_csv(os.getcwd() + '/data/meta_base.csv', index=False)\n",
    "SHAP_base.to_csv(os.getcwd() + '/data/SHAP_base.csv', index=False)\n",
    "\n",
    "# Save alerts\n",
    "data['X_alert'].to_csv(os.getcwd() + '/data/X_alert.csv', index=False)\n",
    "data['X_alert_decoded'].to_csv(os.getcwd() + '/data/X_alert_decoded.csv', index=False)\n",
    "meta_alert.to_csv(os.getcwd() + '/data/meta_alert.csv', index=False)\n",
    "SHAP_alert.to_csv(os.getcwd() + '/data/SHAP_alert.csv', index=False)\n",
    "\n",
    "# Save training data separately\n",
    "data['X_train'].to_csv(os.getcwd() + '/data/X_train.csv', index=False)\n",
    "\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>TP</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>TP</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>TP</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>TP</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>TP</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>TP</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>FN</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>TN</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>TP</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    performance  score\n",
       "500          TN   0.20\n",
       "501          TN   0.04\n",
       "502          TN   0.04\n",
       "503          TN   0.04\n",
       "504          FP   0.55\n",
       "505          TP   0.81\n",
       "506          FN   0.33\n",
       "507          TP   0.54\n",
       "508          TN   0.27\n",
       "509          FP   0.62\n",
       "510          TN   0.04\n",
       "511          TN   0.24\n",
       "512          TN   0.33\n",
       "513          FN   0.18\n",
       "514          FP   0.52\n",
       "515          FN   0.16\n",
       "516          TN   0.09\n",
       "517          FN   0.48\n",
       "518          TP   0.56\n",
       "519          TN   0.24\n",
       "520          TN   0.39\n",
       "521          FN   0.18\n",
       "522          TN   0.10\n",
       "523          TN   0.16\n",
       "524          TN   0.17\n",
       "525          TN   0.28\n",
       "526          TP   0.63\n",
       "527          TN   0.38\n",
       "528          TN   0.25\n",
       "529          FN   0.39\n",
       "..          ...    ...\n",
       "720          TP   0.72\n",
       "721          TN   0.26\n",
       "722          FN   0.04\n",
       "723          TN   0.17\n",
       "724          TN   0.20\n",
       "725          TP   0.54\n",
       "726          FN   0.44\n",
       "727          FP   0.53\n",
       "728          TN   0.38\n",
       "729          TN   0.31\n",
       "730          FP   0.58\n",
       "731          TN   0.40\n",
       "732          TN   0.25\n",
       "733          TN   0.22\n",
       "734          TN   0.03\n",
       "735          TN   0.21\n",
       "736          TN   0.19\n",
       "737          TN   0.06\n",
       "738          TN   0.29\n",
       "739          FP   0.53\n",
       "740          TN   0.17\n",
       "741          FP   0.62\n",
       "742          TN   0.05\n",
       "743          TN   0.37\n",
       "744          TN   0.23\n",
       "745          TN   0.37\n",
       "746          TN   0.02\n",
       "747          TN   0.30\n",
       "748          TN   0.20\n",
       "749          TP   0.60\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = meta_base.iloc[500:]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.744"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['performance'].map({'TP' : 1, 'TN' : 1, 'FP' : 0, 'FN' : 0}).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
