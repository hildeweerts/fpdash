{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hildeweerts/anaconda/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# shap\n",
    "import shap \n",
    "\n",
    "# Directories\n",
    "data_dir = os.getcwd() + '/data'\n",
    "code_dir = os.getcwd() + '/fpdash'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# custom modules\n",
    "from cbr import prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing...\n",
      "...Used all 1000 samples from dataset 31.\n",
      "...Filled missing values.\n",
      "...Decoded to original feature values.\n",
      "...Scaled data.\n",
      "Preprocessing done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hildeweerts/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/hildeweerts/Projects/fpdash/fpdash/cbr/prep.py:78: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  X_train = pd.DataFrame(scaler.transform(X_train), columns=list(X_train))\n",
      "/Users/hildeweerts/Projects/fpdash/fpdash/cbr/prep.py:79: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  X_test = pd.DataFrame(scaler.transform(X_test), columns=list(X_test))\n"
     ]
    }
   ],
   "source": [
    "data = prep.openmlwrapper(data_id=31, random_state=1, n_samples = 2000, verbose=True, scale=True, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test data further\n",
    "* train: \n",
    "    - true class known\n",
    "    - used during training\n",
    "* test pre: \n",
    "    - true class known\n",
    "    - not used during training\n",
    "* test post: \n",
    "    - true class unknown\n",
    "    - not used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X_test_pre'], data['X_test_post'], data['y_test_pre'], data['y_test_post'] = train_test_split(data['X_test'], \n",
    "                                                                                                    data['y_test'], \n",
    "                                                                                                    random_state=1,\n",
    "                                                                                                    test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:    1.00\n",
      "Test (pre) Accuracy:  0.74\n",
      "Test (post) Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 100, n_jobs=-2, random_state=1)\n",
    "clf.fit(data['X_train'], np.array(data['y_train']).ravel())\n",
    "print('Training Accuracy:    %.2f' % clf.score(data['X_train'], data['y_train']))\n",
    "print('Test (pre) Accuracy:  %.2f' % clf.score(data['X_test_pre'], data['y_test_pre']))\n",
    "print('Test (post) Accuracy: %.2f' % clf.score(data['X_test_post'], data['y_test_post']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create casebase and alerts\n",
    "* The case base consists of instances from the training dataset and test-pre dataset.\n",
    "* The alert data consists of instances from test-post dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_indices = data['X_test_pre'].index\n",
    "post_indices = data['X_test_post'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Base\n",
    "data['X_base'] = pd.concat([data['X_train'], data['X_test_pre']]).reset_index(drop=True)\n",
    "data['y_base'] = pd.concat([data['y_train'], data['y_test_pre']]).reset_index(drop=True)\n",
    "data['X_base_decoded'] = pd.concat([data['X_train_decoded'], data['X_test_decoded'].iloc[pre_indices]]).reset_index(drop=True)\n",
    "\n",
    "# Alerts\n",
    "data['X_alert'] = data['X_test_post'].copy().reset_index(drop=True)\n",
    "data['y_alert'] = data['y_test_post'].copy().reset_index(drop=True)\n",
    "data['X_alert_decoded'] = data['X_test_decoded'].iloc[post_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve metadata\n",
    "* Retrieve prediction probabilities (case base + alerts)\n",
    "* Retrieve historical performance (case base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction probabilities\n",
    "y_base_score = [i[1] for i in clf.predict_proba(data['X_base'])]\n",
    "y_alert_score = [i[1] for i in clf.predict_proba(data['X_alert'])]\n",
    "\n",
    "# Compute performance for cases in de case base\n",
    "y_base_pred = clf.predict(data['X_base'])\n",
    "base_performance = []\n",
    "for pred, true in zip(y_base_pred, data['y_base'].values.ravel()):\n",
    "    if (pred==1) and (true==1):\n",
    "        base_performance.append('TP')\n",
    "    elif (pred==1) and (true==0):\n",
    "        base_performance.append('FP')\n",
    "    elif (pred==0) and (true==0):\n",
    "        base_performance.append('TN')\n",
    "    elif (pred==0) and (true==1):\n",
    "        base_performance.append('FN')\n",
    "\n",
    "# gather metadata\n",
    "meta_base = pd.DataFrame({'performance' : base_performance, 'score' : y_base_score})\n",
    "meta_alert = pd.DataFrame({'score' : y_alert_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained.\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "SHAP_base = pd.DataFrame(explainer.shap_values(X=data['X_base'])[1], columns=list(data['X_base']))\n",
    "SHAP_alert = pd.DataFrame(explainer.shap_values(X=data['X_alert'])[1], columns=list(data['X_alert']))\n",
    "print('Explained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized nearest neighbor.\n"
     ]
    }
   ],
   "source": [
    "# Find nearest SHAP neighbors\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='brute', metric='euclidean', n_jobs=-2)\n",
    "nn.fit(SHAP_base)\n",
    "print('Initialized nearest neighbor.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save classifier\n",
    "with open(os.getcwd() + '/data/clf.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# Save nearest neighbor on SHAP\n",
    "with open(os.getcwd() + '/data/nn.pickle', 'wb') as handle:\n",
    "    pickle.dump(nn, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Save case base\n",
    "data['X_base'].to_csv(os.getcwd() + '/data/X_base.csv', index=False)\n",
    "data['X_base_decoded'].to_csv(os.getcwd() + '/data/X_base_decoded.csv', index=False)\n",
    "meta_base.to_csv(os.getcwd() + '/data/meta_base.csv', index=False)\n",
    "SHAP_base.to_csv(os.getcwd() + '/data/SHAP_base.csv', index=False)\n",
    "\n",
    "# Save alerts\n",
    "data['X_alert'].to_csv(os.getcwd() + '/data/X_alert.csv', index=False)\n",
    "data['X_alert_decoded'].to_csv(os.getcwd() + '/data/X_alert_decoded.csv', index=False)\n",
    "meta_base.to_csv(os.getcwd() + '/data/meta_alert.csv', index=False)\n",
    "SHAP_alert.to_csv(os.getcwd() + '/data/SHAP_alert.csv', index=False)\n",
    "\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
